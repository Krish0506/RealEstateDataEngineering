# Real Estate Data Engineering Streaming Pipeline

In this project, you'll build a complete Real Estate Data Engineering Streaming Pipeline, starting from data gathering and ingestion to processing and storage. This project utilizes various industry-standard tools and frameworks to create an efficient and scalable pipeline for handling real estate data streams.

# Overview

The pipeline architecture includes the following components:

- Large Language Models (ChatGPT): For extracting property details.
- WebSocket and Chrome DevTools Protocol (CDP): For data gathering and automation.
- Apache Kafka: For streaming property information in real-time.
- Apache Spark (Master-Worker Architecture): For processing data streams.
- Apache Zookeeper: For managing Kafka cluster coordination.
- Confluent Control Center: For monitoring and managing Kafka clusters.
- Cassandra: For storing processed data.

We will demonstrate how to automate the data collection process, stream it in real time, process the data using Apache Spark, and finally store the data in Cassandra.

# Tools Used

- Bright Data: Web Scrapping
- Docker: To containerize services like Kafka, Spark, and Cassandra.
- Apache Kafka: For creating a real-time streaming platform.
- Apache Spark: For distributed data processing.
- Zookeeper: For Kafka cluster coordination.
- Confluent Control Center: To manage and monitor Kafka clusters.
- Cassandra: As the database for storing real estate data.

# Key Steps

- Data Gathering: Automate the manual process of gathering property data using WebSocket and Chrome DevTools Protocol (CDP).
- Data Streaming: Stream real-time property data into Kafka.
- Data Processing: Use Apache Spark for processing real-time data streams.
- Data Storage: Store the processed data in Cassandra for future retrieval and analysis.

#DataEngineering #Kafka #ApacheSpark #Cassandra #OpenAI #ChatGPT #Docker #ETLPipeline #DataPipeline #StreamingData #RealTimeAnalytics #Spark
